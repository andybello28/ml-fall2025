{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tE4eJZzH0uGi",
        "outputId": "75a7981a-e6db-415a-9757-d90a0aadc577"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.7972824141696345"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "y = np.loadtxt('data/wine_data.txt', dtype=int)\n",
        "\n",
        "x = np.arange(1, len(y) + 1)\n",
        "\n",
        "def find_c_star(x, y, rate):\n",
        "  #Generate a random probability from a normal distribution of our model generating y = +1 as a label\n",
        "  c = np.random.rand()\n",
        "  c_prev = c + 1\n",
        "  while not np.isclose(c, c_prev):\n",
        "    c_prev = c\n",
        "    c += rate * np.mean(y / (y * c - ((y - 1) / 2)))\n",
        "  return float(c)\n",
        "\n",
        "\n",
        "find_c_star(x, y, .01)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEF1usSSdBCC",
        "outputId": "d1f08905-0d38-47e2-da58-446b9a36cb99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Posterior Distribution: [0.0, 5.316029252223897e-78, 2.9713139621418206e-62, 3.9484916743910903e-53, 1.015455062763894e-46, 8.49479483556155e-42, 8.123788449593828e-38, 1.7363126076924741e-34, 1.237457840752838e-31, 3.8065510220018783e-29, 6.027415902635356e-27, 5.570264805689705e-25, 3.2962857086926764e-23, 1.3400955384232744e-21, 3.95317750451714e-20, 8.836076241235773e-19, 1.549596909763671e-17, 2.1937967597146792e-16, 2.5670069734922892e-15, 2.5320364527344614e-14, 2.140683071958844e-13, 1.5733637051601132e-12, 1.0176106089246151e-11, 5.852920268891885e-11, 3.0211192203317e-10, 1.4106947659171033e-09, 6.000832240332104e-09, 2.3398452853370644e-08, 8.408842330981289e-08, 2.7987847212354987e-07, 8.66498064115605e-07, 2.505014556250673e-06, 6.785798573574681e-06, 1.7277772705546562e-05, 4.1464915975819825e-05, 9.403019326564316e-05, 0.0002019410145726853, 0.0004115574008639306, 0.0007973945133367461, 0.001471159070816259, 0.0025883411945722542, 0.004348326730867841, 0.006983309727140735, 0.010731973170588105, 0.0157965533661267, 0.022286550421089908, 0.030158171513005965, 0.039163866166007685, 0.04882878834761411, 0.05846873988868073, 0.06725640499089001, 0.0743307150196771, 0.07893117814943383, 0.08052924507184359, 0.0789260876794179, 0.07429231758659238, 0.06713893410736155, 0.05822613712733719, 0.04843212488844452, 0.038612782509130285, 0.029482823919424573, 0.02154026300894791, 0.015042561660085942, 0.010029132479513043, 0.0063751348141093565, 0.003857795502948141, 0.0022185585806113154, 0.0012101882539430199, 0.0006248194934454361, 0.0003046033218669421, 0.00013983899965039335, 6.0274220052651135e-05, 2.4309890346880763e-05, 9.139905759926106e-06, 3.1897872393539314e-06, 1.0283964843535684e-06, 3.04636521190282e-07, 8.240431297802316e-08, 2.0212179999903363e-08, 4.459394633184618e-09, 8.76823786274331e-10, 1.520052056626976e-10, 2.2944316888350975e-11, 2.9714704563529474e-12, 3.244700099219192e-13, 2.9256707813729896e-14, 2.1240699201176943e-15, 1.2039020755028728e-16, 5.126646875283233e-18, 1.5626429990986918e-19, 3.2032159347123937e-21, 4.0674755649640884e-23, 2.862315180560349e-25, 9.548548755839229e-28, 1.2012080244912439e-30, 3.996451069532194e-34, 1.9405648448845475e-38, 4.511636524640815e-44, 4.112680991434785e-52, 5.004936014604044e-66, 0.0]\n",
            "Max likelihood of heads probability 0.53\n",
            "Maximum a posteriori estimate: 0.53\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "tosses = np.loadtxt('data/bayes_tosses.txt', dtype=int)\n",
        "\n",
        "def find_posterior_distribution(t):\n",
        "  num_heads = np.count_nonzero(t)\n",
        "  likelihoods = []\n",
        "  probability_generating_y = 0\n",
        "  for k in range(0, 101):\n",
        "    theta = .01 * k\n",
        "    probability_generating_y += (theta ** num_heads) * ((1 - theta) ** (len(t) - num_heads))\n",
        "  for k in range(0, 101):\n",
        "    theta = .01 * k\n",
        "    likelihood = ((theta ** num_heads) * ((1-theta) ** (len(t) - num_heads))) / probability_generating_y\n",
        "    likelihoods.append(likelihood)\n",
        "  return likelihoods\n",
        "\n",
        "posterior_distribution = find_posterior_distribution(tosses)\n",
        "print(f\"Posterior Distribution: {posterior_distribution}\")\n",
        "print(f\"Max likelihood of heads probability {np.count_nonzero(tosses) / len(tosses)}\")\n",
        "print(f\"Maximum a posteriori estimate: {np.argmax(posterior_distribution) * .01}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7G8jdM1jVJN",
        "outputId": "36e40725-bfc0-4338-97a0-981de082d508"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Posterior Distribution: [np.float64(0.0), np.float64(9.963366205825933e-79), np.float64(6.131519570017892e-63), np.float64(8.948822817463724e-54), np.float64(2.521301390885639e-47), np.float64(2.3049450901323832e-42), np.float64(2.4028363538479677e-38), np.float64(5.584260508499164e-35), np.float64(4.316727060631661e-32), np.float64(1.436668348437092e-29), np.float64(2.4551101531986866e-27), np.float64(2.442557985038336e-25), np.float64(1.5521631555223783e-23), np.float64(6.759367037801777e-22), np.float64(2.130534607186676e-20), np.float64(5.075607206961585e-19), np.float64(9.463413777062185e-18), np.float64(1.4208239835654938e-16), np.float64(1.7587333614149607e-15), np.float64(1.8305695564376785e-14), np.float64(1.6290193712879962e-13), np.float64(1.2571145628274958e-12), np.float64(8.515565145077418e-12), np.float64(5.116875395369831e-11), np.float64(2.752420251151371e-10), np.float64(1.3360073482433337e-09), np.float64(5.892918741623682e-09), np.float64(2.376640795630806e-08), np.float64(8.812199436908057e-08), np.float64(3.018586107088155e-07), np.float64(9.594048319404418e-07), np.float64(2.8402662445557113e-06), np.float64(7.859203250959056e-06), np.float64(2.03895825514554e-05), np.float64(4.973460268718695e-05), np.float64(0.0001143448847407118), np.float64(0.0002483474809001669), np.float64(0.0005105822558435047), np.float64(0.0009954578725176317), np.float64(0.001843477666908501), np.float64(0.0032474512424357704), np.float64(0.005448794338961924), np.float64(0.008717881209166892), np.float64(0.013314193985063337), np.float64(0.01942663427577555), np.float64(0.02710143359982551), np.float64(0.03617286186823717), np.float64(0.046217466558615895), np.float64(0.05655272079787105), np.float64(0.0662937418280255), np.float64(0.07446770235206701), np.float64(0.08016824659142717), np.float64(0.08271736539526252), np.float64(0.08179561711984051), np.float64(0.07750675284055857), np.float64(0.07035900650640638), np.float64(0.06116758340226312), np.float64(0.05090380096544673), np.float64(0.04052895193751145), np.float64(0.030851538677497823), np.float64(0.022435871948605333), np.float64(0.015572820706928388), np.float64(0.010306109529863428), np.float64(0.006495420279087785), np.float64(0.0038933053009038506), np.float64(0.002215989781343163), np.float64(0.001195675081288995), np.float64(0.0006104121951530307), np.float64(0.00029421654622371874), np.float64(0.0001335683853549183), np.float64(5.695975744841484e-05), np.float64(2.2748693942590616e-05), np.float64(8.48022114737471e-06), np.float64(2.939540642649026e-06), np.float64(9.434682829712519e-07), np.float64(2.790411626419777e-07), np.float64(7.563904231281668e-08), np.float64(1.867603269921737e-08), np.float64(4.170928377181586e-09), np.float64(8.35785126052987e-10), np.float64(1.4888278147087575e-10), np.float64(2.3324819739753024e-11), np.float64(3.173780697073463e-12), np.float64(3.6959782791109457e-13), np.float64(3.619953824146938e-14), np.float64(2.9203752690342307e-15), np.float64(1.8922609148407215e-16), np.float64(9.548092987615389e-18), np.float64(3.610658473639429e-19), np.float64(9.748876565460343e-21), np.float64(1.7657819584220917e-22), np.float64(1.9762693409117505e-24), np.float64(1.2227093125373243e-26), np.float64(3.577189812769616e-29), np.float64(3.9367329308151597e-32), np.float64(1.142929928617357e-35), np.float64(4.8307538123951e-40), np.float64(9.751619657391682e-46), np.float64(7.699062449568961e-54), np.float64(8.094607221613141e-68), np.float64(0.0)]\n",
            "Max likelihood of heads probability 0.53\n",
            "Maximum a posteriori estimate: 0.52\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "\n",
        "tosses = np.loadtxt('ps4Tosses2025.txt', dtype=int)\n",
        "\n",
        "def find_posterior_distribution(t):\n",
        "  num_heads = np.count_nonzero(t)\n",
        "  likelihoods = []\n",
        "  probability_generating_y = 0\n",
        "  gaussian_pdf = []\n",
        "  for k in range(0, 101):\n",
        "    theta = .01 * k\n",
        "    probability = norm.pdf(theta, loc=0.4, scale=0.2)\n",
        "    gaussian_pdf.append(probability)\n",
        "  sum = np.sum(gaussian_pdf)\n",
        "  gaussian_pdf /= sum\n",
        "\n",
        "  for k in range(0, 101):\n",
        "    theta = .01 * k\n",
        "    probability_generating_y += ((theta ** num_heads) * ((1 - theta) ** (len(t) - num_heads))) * gaussian_pdf[k]\n",
        "  for k in range(0, 101):\n",
        "    theta = .01 * k\n",
        "    likelihood = ((theta ** num_heads) * ((1-theta) ** (len(t) - num_heads)) * gaussian_pdf[k]) / probability_generating_y\n",
        "    likelihoods.append(likelihood)\n",
        "  return likelihoods\n",
        "\n",
        "posterior_distribution = find_posterior_distribution(tosses)\n",
        "print(f\"Posterior Distribution: {posterior_distribution}\")\n",
        "print(f\"Max likelihood of heads probability {np.count_nonzero(tosses) / len(tosses)}\")\n",
        "print(f\"Maximum a posteriori estimate: {np.argmax(posterior_distribution) * .01}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LL1RiBGQStYn",
        "outputId": "cdaf57d8-50d1-4f57-afe2-4ee959117bee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KFold cross validation\n",
            "\n",
            "Fold 1:\n",
            "Out of Sample Accuracy: 0.8222\n",
            "Class Distribution\n",
            "Train Class 1: 0.1053\n",
            "Train Class 2: 0.5338\n",
            "Train Class 3: 0.3609\n",
            "Test Class 1: 1.0000\n",
            "Test Class 2: 0.0000\n",
            "Test Class 3: 0.0000\n",
            "Fold 2:\n",
            "Out of Sample Accuracy: 0.8444\n",
            "Class Distribution\n",
            "Train Class 1: 0.3383\n",
            "Train Class 2: 0.3008\n",
            "Train Class 3: 0.3609\n",
            "Test Class 1: 0.3111\n",
            "Test Class 2: 0.6889\n",
            "Test Class 3: 0.0000\n",
            "Fold 3:\n",
            "Out of Sample Accuracy: 1.0000\n",
            "Class Distribution\n",
            "Train Class 1: 0.4403\n",
            "Train Class 2: 0.2313\n",
            "Train Class 3: 0.3284\n",
            "Test Class 1: 0.0000\n",
            "Test Class 2: 0.9091\n",
            "Test Class 3: 0.0909\n",
            "Fold 4:\n",
            "Out of Sample Accuracy: 0.0000\n",
            "Class Distribution\n",
            "Train Class 1: 0.4403\n",
            "Train Class 2: 0.5299\n",
            "Train Class 3: 0.0299\n",
            "Test Class 1: 0.0000\n",
            "Test Class 2: 0.0000\n",
            "Test Class 3: 1.0000\n",
            "\n",
            "Mean k-fold cross validation accuracy: 0.6667\n",
            "Standard Deviation k-fold cross validation accuracy: 0.3909\n",
            "\n",
            "Stratified cross validation\n",
            "\n",
            "Fold 1:\n",
            "Out of Sample Accuracy: 0.9111\n",
            "Class Distribution\n",
            "Train Class 1: 0.3308\n",
            "Train Class 2: 0.3985\n",
            "Train Class 3: 0.2707\n",
            "Test Class 1: 0.3333\n",
            "Test Class 2: 0.4000\n",
            "Test Class 3: 0.2667\n",
            "Fold 2:\n",
            "Out of Sample Accuracy: 0.9556\n",
            "Class Distribution\n",
            "Train Class 1: 0.3308\n",
            "Train Class 2: 0.3985\n",
            "Train Class 3: 0.2707\n",
            "Test Class 1: 0.3333\n",
            "Test Class 2: 0.4000\n",
            "Test Class 3: 0.2667\n",
            "Fold 3:\n",
            "Out of Sample Accuracy: 0.9773\n",
            "Class Distribution\n",
            "Train Class 1: 0.3284\n",
            "Train Class 2: 0.4030\n",
            "Train Class 3: 0.2687\n",
            "Test Class 1: 0.3409\n",
            "Test Class 2: 0.3864\n",
            "Test Class 3: 0.2727\n",
            "Fold 4:\n",
            "Out of Sample Accuracy: 1.0000\n",
            "Class Distribution\n",
            "Train Class 1: 0.3358\n",
            "Train Class 2: 0.3955\n",
            "Train Class 3: 0.2687\n",
            "Test Class 1: 0.3182\n",
            "Test Class 2: 0.4091\n",
            "Test Class 3: 0.2727\n",
            "\n",
            "Mean stratified cross validation accuracy: 0.9610\n",
            "Standard Deviation stratified cross validation accuracy: 0.0328\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "\n",
        "D = fetch_openml(\"wine\", version=1, as_frame=False)\n",
        "X = D.data\n",
        "y = D.target\n",
        "\n",
        "classes = ['1', '2', '3']\n",
        "\n",
        "kf = KFold(n_splits= 4)\n",
        "kf_accuracies = []\n",
        "fold_num = 1\n",
        "print(\"KFold cross validation\\n\")\n",
        "#For each train, test split in the KFold cv, train model and report out of sample error\n",
        "for train, test in kf.split(X):\n",
        "  print(f\"Fold {fold_num}:\")\n",
        "  clf = GaussianNB()\n",
        "  clf.fit(X[train], y[train])\n",
        "  out_of_sample_accuracy = clf.score(X[test], y[test])\n",
        "  print(f\"Out of Sample Accuracy: {out_of_sample_accuracy:.4f}\")\n",
        "  print(\"Class Distribution\")\n",
        "  for cl in classes:\n",
        "    count = np.count_nonzero(y[train] == cl)\n",
        "    print(f\"Train Class {cl}: {count / len(y[train]):.4f}\")\n",
        "  for cl in classes:\n",
        "    count = np.count_nonzero(y[test] == cl)\n",
        "    print(f\"Test Class {cl}: {count / len(y[test]):.4f}\")\n",
        "  kf_accuracies.append(out_of_sample_accuracy)\n",
        "  fold_num += 1\n",
        "\n",
        "mean_kf_accuracy = np.mean(kf_accuracies)\n",
        "sd_kf_accuracy = np.std(kf_accuracies)\n",
        "print(f\"\\nMean k-fold cross validation accuracy: {mean_kf_accuracy:.4f}\")\n",
        "print(f\"Standard Deviation k-fold cross validation accuracy: {sd_kf_accuracy:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "skf = StratifiedKFold(n_splits=4)\n",
        "skf_accuracies = []\n",
        "fold_num = 1\n",
        "print(\"\\nStratified cross validation\\n\")\n",
        "for train, test in skf.split(X, y):\n",
        "  print(f\"Fold {fold_num}:\")\n",
        "  clf = GaussianNB()\n",
        "  clf.fit(X[train], y[train])\n",
        "  out_of_sample_accuracy = clf.score(X[test], y[test])\n",
        "  print(f\"Out of Sample Accuracy: {out_of_sample_accuracy:.4f}\")\n",
        "  print(\"Class Distribution\")\n",
        "  for cl in classes:\n",
        "    count = np.count_nonzero(y[train] == cl)\n",
        "    print(f\"Train Class {cl}: {count / len(y[train]):.4f}\")\n",
        "  for cl in classes:\n",
        "    count = np.count_nonzero(y[test] == cl)\n",
        "    print(f\"Test Class {cl}: {count / len(y[test]):.4f}\")\n",
        "  skf_accuracies.append(out_of_sample_accuracy)\n",
        "  fold_num += 1\n",
        "\n",
        "mean_skf_accuracy = np.mean(skf_accuracies)\n",
        "sd_skf_accuracy = np.std(skf_accuracies)\n",
        "print(f\"\\nMean stratified cross validation accuracy: {mean_skf_accuracy:.4f}\")\n",
        "print(f\"Standard Deviation stratified cross validation accuracy: {sd_skf_accuracy:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOSh-K76GoRf",
        "outputId": "43457dc5-97a3-45a0-8f16-23fa5a0a870d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Degree 1\n",
            "In-sample error: 0.0207\n",
            "Out-of-sample error: 0.0265\n",
            "Degree 2\n",
            "In-sample error: 0.0043\n",
            "Out-of-sample error: 0.0068\n",
            "Degree 4\n",
            "In-sample error: 0.0005\n",
            "Out-of-sample error: 0.0055\n",
            "Degree 8\n",
            "In-sample error: 0.0000\n",
            "Out-of-sample error: 0.0080\n",
            "Degree 16\n",
            "In-sample error: 0.0000\n",
            "Out-of-sample error: 0.0116\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "\n",
        "D = fetch_openml(\"pendigits\", version=1, as_frame=False)\n",
        "X = D.data\n",
        "y = D.target\n",
        "\n",
        "for p in range(0, 5):\n",
        "  k = 2 ** p\n",
        "  clf = SVC(kernel=\"poly\", degree = k)\n",
        "  clf.fit(X, y)\n",
        "  print(f\"Degree {k}\")\n",
        "  print(f\"In-sample error: {1 - clf.score(X, y):.4f}\")\n",
        "  print(f\"Out-of-sample error: {1 - np.mean(cross_val_score(clf, X, y)):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8w_vA5ZML90n",
        "outputId": "e0864879-e523-4269-9ba6-2ce644ea5ec9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Degree 1\n",
            "In-sample error: 0.0217\n",
            "Out-of-sample error: 0.0267\n",
            "Degree 2\n",
            "In-sample error: 0.0083\n",
            "Out-of-sample error: 0.0130\n",
            "Degree 4\n",
            "In-sample error: 0.0069\n",
            "Out-of-sample error: 0.0096\n"
          ]
        }
      ],
      "source": [
        "#Shows how important the kernel trick is by comparing it with\n",
        "#regular feature expansion using PolynomialFeatures\n",
        "import numpy as np\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "D = fetch_openml(\"pendigits\", version=1, as_frame=False)\n",
        "X = D.data\n",
        "y = D.target\n",
        "\n",
        "for p in range(0, 5):\n",
        "  k = 2 ** p\n",
        "  poly = PolynomialFeatures(degree=k)\n",
        "  X_poly = poly.fit_transform(X)\n",
        "  clf = SVC(kernel=\"poly\", degree = 1)\n",
        "  clf.fit(X_poly, y)\n",
        "  print(f\"Degree {k}\")\n",
        "  print(f\"In-sample error: {1 - clf.score(X_poly, y):.4f}\")\n",
        "  print(f\"Out-of-sample error: {1 - np.mean(cross_val_score(clf, X_poly, y)):.4f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
